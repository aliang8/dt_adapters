# shared configs 

# training config
seed: 1                                             # random seed
save_every: 10                                      # epochs between saving model checkpoints
eval_every: 1                                       # epochs between evaluating policy
num_steps_per_epoch: 200                            # number of gradient updates per online rollout
skip_first_eval: False                              # skip first evalution step
num_epochs: 100                                     # number of training epochs
batch_size: 128                                   

lr: 1e-4                                            
weight_decay: 1e-4
alpha_lr: 1e-4                                      # learning rate for auto-tuning alpha
alpha_weight_decay: 1e-4                
warmup_steps: 10000                                 # warmup steps for scheduler
num_data_workers: 1                                 # for data loading

# logging
exp_name: test                                      # exp name, used for folder path and wandb group name
log_to_wandb: False                                 # should we log the metrics to wandb
log_outputs: True                                   # should we create a new folder and store exp output
save_best_model: True                               # save models with the highest success rates

# video configs
log_eval_videos: True                               # should we videos of eval rollouts
log_eval_videos_every: 10                           # number of epochs between every video save
fps: 20           
image_height: 128                                   # resizing videos to fixed dimension for wandb logging
image_width: 128

# online_finetuning
online_training: False                              # for online finetune  
num_online_rollouts: 1                              # number of online rollouts per epoch
train_on_offline_data: True                         # should we continue training on offline data during online fine-tuning
num_warmup_rollouts: 10                             # collect some initial trajectories to populate replay buffer
target_return: 500                                  # conditioning RTG for inference

load_from_ckpt: False                               # load pretrained model weights 
freeze_backbone: False                              # freeze part of the pretrained model weights 

# eval parameters
num_eval_rollouts: 10                               # number of rollouts per evaluation step
num_processes: 5                                    # number of processes for collecting eval rollouts

model_ckpt_dir:                                     # where to look for model checkpoint
mode: train