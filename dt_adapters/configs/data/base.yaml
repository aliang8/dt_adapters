context_len: 20                                   # number of timesteps the DT model can attend to 
state_dim: ???                                
act_dim: ???                                     
max_ep_len: ???                                   # max number of steps allowed in env
scale: 1000                                       # return normalization for online fine-tuning
data_dir: /data/anthony/dt_adapters/data/         # root folder of dataset
data_file: ???                                    # data hdf5 file name

# visual encoder
image_size: 64                                    # resize dimension for image input
vision_backbone: clip                             # which model to use for extracting visual features [clip|resnet]
vision_mdl_key: openai/clip-vit-base-patch32      
observation_mode: state                           # what modality of information to use as state representation [state|image|state_image]