_target_: dt_adapters.models.transformer_policy.TransformerPolicy
model_cls: transformer

max_length: ${data.context_length}
max_episode_length: 500                               # maximum rollout length
action_tanh: True                                     # apply tanh to the action prediction
log_std_min: -20                                      # bound the std prediction for stochastic action
log_std_max: 2

# these two are environment specific
state_dim: 39
act_dim: 4

hidden_size: 768 

# configuration for the backbone model
gpt2_cfg:
  n_layer: 4                                          # number of transformer layers
  n_head: 4                                           # number of MHA 
  dropout: 0.1                                        # dropout
  n_embd: ${model.hidden_size}

device: cuda 