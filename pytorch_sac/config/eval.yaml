defaults:
    - agent: sac

env: window-open-v2-goal-observable
random_hand_init: false

# this needs to be specified manually
experiment: mw-dc

num_train_steps: 1e6 # 1 million training steps
replay_buffer_capacity: ${num_train_steps}
max_episode_steps: 500
num_seed_steps: 5000

eval_frequency: 10000
save_every_steps: 10000
num_eval_episodes: 10

device: cuda

# logger
log_frequency: 10000
log_save_tb: false
log_to_wandb: false

# video recorder
save_video: true

seed: 0
# experiment_dir: /home/anthony/dt_adapters/pytorch_sac/outputs/
experiment_dir: /home/anthony/dt_adapters/data/local/experiment/
restore_from_checkpoint: true 

# for eval
num_rollouts: 10

demos_per_env: 200
data_dir: /home/anthony/dt_adapters/pytorch_sac/data/
num_processes: 5 # multiprocessing for dataset generation
dataset_file: trajectories_no_image_random_init.hdf5
mp: false
debug: false

model: single_task_sac 

# for decision transformer 
dt: 
    hidden_size: 768
    n_layer: 4
    n_head: 4
    dropout: 0.1
    max_length: 50
    max_ep_len: 5000
    act_dim: 4
    pos_hand: 3
    obs_obj_max_len: 14
    gripper_distance_apart: 1
    goal_pos: 3
    state_dim: 39
    component_hidden_size: 256

    experiment_dir: /home/anthony/dt_adapters/outputs
    experiment_name: test_train_dt

    num_steps_per_iter: 20
    data_file: /home/anthony/dt_adapters/pytorch_sac/data/trajectories_no_image.hdf5